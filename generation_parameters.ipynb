{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPvkgUBbCstbR72yEW+oE1M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dileep9968/mastery-in-llm/blob/main/generation_parameters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ir9kQQjIRHaw"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "21Bq7R6jb9-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "7CQUDyIhcwaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "tienxYaYc51m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {'role':'user', 'content': 'How do chat templates work?'},\n",
        "    {'role': 'assistant', 'content': 'Chat templates help LLMs like me genereted more cohrent responses by providing a structured way to organize the conversation'},\n",
        "    {'role': 'user', 'content':'How do I use them?'}\n",
        "]\n",
        "print(messages)"
      ],
      "metadata": {
        "id": "Z2y-G0sxc_Kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors='pt').to(device)\n",
        "model_inputs"
      ],
      "metadata": {
        "id": "HPyaaJANd4p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_ids = model.generate(model_inputs)\n",
        "decoded_response = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded_response)"
      ],
      "metadata": {
        "id": "HqcJ6K46eLyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decoded_response[0])"
      ],
      "metadata": {
        "id": "ZDvFrZFRgIXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "id": "zIIbJVoeg4gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max generated token would be 2048\n",
        "generated_ids = model.generate(model_inputs, max_new_tokens =128)\n",
        "response = tokenizer.batch_decode(generated_ids)\n",
        "print(response[0])"
      ],
      "metadata": {
        "id": "HckKyHulhOfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_ids.shape"
      ],
      "metadata": {
        "id": "3LIXQ375h0n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {'role':'user', 'content': 'How do chat templates work?'},\n",
        "    {'role': 'assistant', 'content': 'Chat templates help LLMs like me genereted more cohrent responses by providing a structured way to organize the conversation' * 80},\n",
        "    {'role': 'user', 'content':'How do I use them?'}\n",
        "]"
      ],
      "metadata": {
        "id": "bqRBM8uEh7iL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors='pt').to(device)\n"
      ],
      "metadata": {
        "id": "Mp7J-cK2iLid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors='pt').to(device)\n",
        "generated_ids = model.generate(model_inputs, max_new_tokens =128)\n",
        "response = tokenizer.batch_decode(generated_ids)\n",
        "print(response[0])"
      ],
      "metadata": {
        "id": "ZZsi64W2iYq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {'role':'user', 'content': 'How do chat templates work?'},\n",
        "    {'role': 'assistant', 'content': 'Chat templates help LLMs like me genereted more cohrent responses by providing a structured way to organize the conversation'*70},\n",
        "    {'role': 'user', 'content':'How do I use them?'}\n",
        "]\n",
        "model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors='pt').to(device)\n",
        "generated_ids = model.generate(model_inputs, max_new_tokens =128)\n",
        "response = tokenizer.batch_decode(generated_ids)\n",
        "print(response[0])"
      ],
      "metadata": {
        "id": "3rD8FYX3ipyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_ids.shape"
      ],
      "metadata": {
        "id": "kzqVB2CJjDx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors='pt').to(device)\n",
        "generated_ids = model.generate(model_inputs, max_new_tokens =128, do_sample = True, temperature = 0.1)\n",
        "response = tokenizer.batch_decode(generated_ids)\n",
        "print(response[0])"
      ],
      "metadata": {
        "id": "4xxZyxqMjK_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors='pt').to(device)\n",
        "generated_ids = model.generate(model_inputs, max_new_tokens =128, do_sample = True, temperature = 0.2)\n",
        "response = tokenizer.batch_decode(generated_ids)\n",
        "print(response[0])"
      ],
      "metadata": {
        "id": "B0o3XYR0jYnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors='pt').to(device)\n",
        "generated_ids = model.generate(model_inputs, max_new_tokens =128, do_sample = True, temperature = 1.0, top_p = 0.1)\n",
        "response = tokenizer.batch_decode(generated_ids)\n",
        "print(response[0])"
      ],
      "metadata": {
        "id": "CMjCeDQTjd2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h3U4IHVYjrID"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}